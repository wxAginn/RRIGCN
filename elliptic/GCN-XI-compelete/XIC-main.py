
import argparse
import sys
import time

from experiment import Experiment
from model_dgl import GCN,statistical_information_X
from dataset_dgl import xdataset_
import torch
from config import Logger

recordname = '../log/GCN-XI-res-root_lastactivate'+time.strftime('%Y_%m_%d_%H%M%S',time.localtime())+'.txt'
#sys.stdout = Logger(recordname, sys.stdout)
args = argparse.ArgumentParser()
args.add_argument('--learning_rate', type=float, default=3e-4)
args.add_argument('--epochs', type=int, default=150)
args.add_argument('--batch_size', type=int, default=24)
args.add_argument('--train_set_len', type=float, default=0.8)
args.add_argument('--shuffle', type=bool, default=True)
args.add_argument('--graph_size', type=int, default=50)
args.add_argument('--LRM_layer_num',type=int,default=1)
args.add_argument('--LRM_dimension',type=int,default=498)
args.add_argument('--layer_num',type=int,default=3)
args.add_argument('--alpha',type=float,default=0.2)
args.add_argument('--beta',type=float,default=0.5)
args.add_argument('--gamma',type=float,default=0.5)
args.add_argument('--delta',type=float,default=0.8)
args = args.parse_args(args=[])
print(args)
#alpha+delta=1,beta+gamma=1
device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
train_len=int(args.train_set_len*xdataset_.__len__())
test_len=xdataset_.__len__()-train_len
train_set,test_set=torch.utils.data.random_split(xdataset_,[train_len,test_len])
total_mean, total_var, total_mean_F, total_var_F = statistical_information_X(
    train_set)
'''
normalize
total_mean, total_var = 0.1616426411451675, 0.06743494481569913 
total_mean_F = [0.12157954859110873, -0.0733134746754382, -0.01985039934330235, 0.02633860678948508, -0.015101970291792503, -0.05219532429356467, -0.018272652762237312, -0.07969327303452119, -0.08709054167968622, -0.07970716812336855, -0.07140107913035774, -0.08555575116201518, -0.04148902185613063, -0.03633117600021669, -0.05181247598036091, -0.1169831797526057, -0.08223707894242757, -0.08023048591314866, -0.09111187663719848, -0.08764837645257649, -0.019364165660937346, -0.018895473999562616, -0.06626949622326148, -0.05294365908236325, -0.08959065869538566, -0.059374861895273454, -0.04313108953259686, -0.03987414402671612, -0.0662697556255708, -0.05294499407857007, -0.08959228751445325, -0.05937603892453781, -0.043125185392098996, -0.03986948212200026, -0.10913709941535843, -0.09838753738104926, -0.0980428873378705, -0.10250719715329408, 0.08601161336877966, 0.08565930383976512, -0.07027777267279636, -0.05535434496184908, -0.08435258968502042, -0.06249893608839047, 0.04317416125177837, 0.04321445081130979, -0.07094334257058375, -0.058168077175275654, -0.08575906073578533, -0.06346426541954558, 0.043257544376228425, 0.043548251204266274, -0.021559626483500346, 0.022716982483010028, -0.01754163965689569, 0.004967108087509405, 0.04061675108168668, 0.04034453384621368, -0.10067366997597117, -0.04649950756059974, -0.05594006565742777, -0.054047748578157265, -0.0008113589116005688, -0.0014433715431916017, -0.10067348965929639, -0.046503493873259014, -0.05594685903015759, -0.05405495118326406, -0.000809391238212039, -0.0014532560255536902, -0.11735666924499567, -0.09052308605495225, -0.09403681251342894, -0.09536534809458636, -0.04831557698015112, -0.049203385969328786, -0.0982660578081932, -0.04469482175384073, -0.053644098088210676, -0.05070053122336587, -0.013012663860078485, -0.01457452572176596, -0.09810989946235574, -0.048934396154735375, -0.05857848356554576, -0.05493957976078112, -0.014512227997060183, -0.015993851330402518, -0.05702474101672003, 0.015612477579815504, -0.0040898970206667586, 0.0005643002054328574, 0.026489836345482576, 0.02656901912956681, -0.07741810006112834, -0.0388067329947691, -0.04807214295265531, -0.062100703276549464, -0.037831990435354665, -0.036062702647219516, 0.0032207774055356313, 0.0036129159456063445, -0.10394400748704147, 0.0034308695721044814, -0.04945080680564998, -0.05043379353367702, -0.08422175604167806, -0.049192426743102265, -0.05504882034049046, -0.06396952642870403, -0.04250514990411662, -0.04109170654444635, -0.008910387606262357, 0.036458523328880686, -0.006190514933365582, 0.018970857674736524, -0.04879155192522565, -0.04921688434284665, -0.08082363251573081, -0.02921371450691875, -0.031809671639120886, -0.049735265366718455, -0.04587136909271431, -0.04612714552228163, -0.08985354777838073, -0.06540218818966508, -0.07915505992728626, -0.07001640235265136, 0.04779393937930315, 0.047532593941423826, -0.07541355282754877, -0.04033365453155135, -0.057539644355248755, -0.06192355579760818, -0.011759332923604324, -0.011668892578484297, 0.004139208154077364, 0.011012532490108187, -0.06057927775710872, 0.0034099290550841863, 0.014749878160807776, 0.014457551771831674, -0.08371270441401785, -0.03327198248919997, -0.04679599398727779, -0.05030044867317337, 0.008269007571577789, 0.008554548528977544, -0.002298155191122976, 0.04065320587846438, -0.01695601801728448, 0.02173815173704304, 0.01927429936722402, 0.01921752720130308, -0.07855007804778474, -0.025357760168974744, -0.03719590323548766, -0.03861855965332689, 0.013064731424262883, 0.012657305097205548, -0.022943517236422416, -0.044593683912218414, -0.05050612466228098, -0.06631545037276763, -0.028446630039389645, -0.02888917806175195]
total_var_F=[0.0036102113417553826, 0.01637792956501741, 0.02742959502138008, 0.020265864725601504, 0.028370755509412492, 0.023403091487544568, 0.028160027287615717, 0.016180202976402932, 0.013134212649744506, 0.01492012121759675, 0.017645086327394685, 0.013495460884312074, 0.028243964382687774, 0.028246909684731907, 0.023558325863270464, 0.005418214564577214, 0.015708904015360806, 0.014524953459184715, 0.01190667426337467, 0.012821116242270017, 0.02109745458936826, 0.020991861707655816, 0.017123917500301154, 0.01900814574015135, 0.013232434488364002, 0.017686518480081348, 0.02902690231190036, 0.028936326132755357, 0.017123801505146006, 0.019007678858431066, 0.013231614117546558, 0.017686211715714932, 0.02903770320960387, 0.02894613100579713, 0.007587283622488228, 0.01130341385577008, 0.011521481104534713, 0.009669691577695273, 0.018789171195573894, 0.018805093464318672, 0.01530684482171609, 0.017672193108183378, 0.014778918134458075, 0.016325964214350987, 0.02714464977595961, 0.026736245876693772, 0.015147059643216457, 0.017025680227492155, 0.01426339951135637, 0.016104465651788302, 0.027144446489030614, 0.026722797884471104, 0.021989220957809797, 0.022563573354138187, 0.024678885640053973, 0.022590833834389985, 0.02641117090879863, 0.026282493866083615, 0.010950682837476838, 0.019422376100047153, 0.01754156824257556, 0.01825493424261174, 0.021747130351528206, 0.021602227583557333, 0.010950596916719825, 0.019420880948900134, 0.017539352286276393, 0.018253268952814497, 0.0217539431498564, 0.021613131687200044, 0.005254588237788183, 0.014144289029917802, 0.012483720645516996, 0.011596326371588042, 0.02107955350999111, 0.021044496588415746, 0.01140951330293619, 0.01879598354347993, 0.01718527906115149, 0.01779342260831347, 0.02334037450388302, 0.023456328952877033, 0.011707292138248403, 0.018403046349129173, 0.01666066060025374, 0.017344356105375075, 0.02303490393429976, 0.023118010055546392, 0.019453460786530947, 0.023232249540028168, 0.021967193930048194, 0.022856254486332887, 0.023534232471249635, 0.02336340794617412, 0.014525012208233873, 0.021972250739069134, 0.022092131580815716, 0.017452205246415574, 0.02975640854956075, 0.02942362053516192, 0.018536880583420415, 0.01841770183666672, 0.009113927895050446, 0.018455498963376796, 0.024705495333135444, 0.024524213081067912, 0.01370413939998826, 0.020953045185255783, 0.02092021864117949, 0.01757102854846763, 0.031217429926016674, 0.031084011567253652, 0.020893175081932697, 0.019036179647123275, 0.023820702483831183, 0.020600339801237256, 0.03057403406674444, 0.030542313779058156, 0.0169685347650054, 0.024976364206332907, 0.02512809198244989, 0.02157137178162218, 0.02851668025327995, 0.028284821899550918, 0.012170512260775052, 0.018959372314407443, 0.016079462016960953, 0.01680368586582206, 0.030874477889492918, 0.03097334281877701, 0.016020325293183355, 0.022787287183696152, 0.01911683288870995, 0.01844547891827077, 0.023657765650640625, 0.023413480417534265, 0.01849595494199776, 0.02057279294820497, 0.018498679248350646, 0.019179909047608343, 0.022508455610588698, 0.022388309205763273, 0.014382360970849193, 0.022622492202817056, 0.019865234558740296, 0.019675096144396585, 0.023403804247156365, 0.023361977704382128, 0.020470059533588797, 0.02192061934238433, 0.0216704959692579, 0.021968401804335343, 0.023717976969839478, 0.023747556766283737, 0.015249745828438738, 0.024747729092823335, 0.021136329442049302, 0.021748634590862032, 0.02265499453719263, 0.022592125344217076, 0.024259266937926356, 0.02276288105979621, 0.021209217341286186, 0.018864202196354037, 0.027438051277869267, 0.027442746472958086]
'''
print(total_mean, '\n', total_var, '\n', total_mean_F, '\n', total_var_F)
model = GCN(in_feats=166,
            num_of_nodes=args.graph_size,
            out_feats=2,
            mean_A=total_mean,
            var_A=total_var,
            var_F=total_var_F,
            layer_num=args.layer_num,
            LRM_dimension=args.LRM_dimension,
            LRM_layer_num=args.LRM_layer_num,
            alpha=args.alpha,
            beta=args.beta,
            gamma=args.gamma,
            delta=args.delta).to(device)

optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

criterion = torch.nn.CrossEntropyLoss()

exp = Experiment(model, optimizer, criterion,train_set,test_set,args)

for epoch in range(args.epochs):
    exp.train(epoch)
    exp.test()
