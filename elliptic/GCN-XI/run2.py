
import argparse
import sys
from experiment import Experiment
from model_dgl import GCN,statistical_information_X
from dataset_dgl import xdataset_
import torch
from config import Logger

recordname = '../log/GCN-XI_2021_12_1_ReLU2.txt'
#sys.stdout = Logger(recordname, sys.stdout)
args = argparse.ArgumentParser()
args.add_argument('--learning_rate', type=float, default=3e-4)
args.add_argument('--epochs', type=int, default=150)
args.add_argument('--batch_size', type=int, default=24)
args.add_argument('--train_set_len', type=float, default=0.8)
args.add_argument('--shuffle', type=bool, default=True)
args.add_argument('--graph_size', type=int, default=50)
args = args.parse_args(args=[])
print(args)
train_len=int(args.train_set_len*xdataset_.__len__())
test_len=xdataset_.__len__()-train_len
train_set,test_set=torch.utils.data.random_split(xdataset_,[train_len,test_len])
total_mean, total_var, total_mean_F, total_var_F = statistical_information_X(
    train_set)
'''
total_mean, total_var = 3.6518664847512335e-05, 4.522542972417184e-05 
total_mean_F = [0.002106552071398657, -3.4439363785674577e-06, 3.0894441014873945e-05, 1.6148149181405387e-05, 2.8771980586182656e-05, 3.4705236687248835e-05, 2.908639091383668e-05, 5.585816413270613e-06, -5.113175731091687e-06, -4.179535606658776e-06, 3.249462846229951e-06, -4.9860830165558575e-06, -3.520406805274017e-06, -6.500694892463501e-07, 3.509048266726235e-05, 2.4273706132051317e-06, 3.2539420156989516e-06, -4.31097068875505e-06, -7.542408396442831e-06, -7.600611375951941e-06, -1.2840812165341203e-05, -1.2720732609323399e-05, 8.358055852868988e-06, 8.200091314769595e-06, 2.2434041891576767e-06, 7.97237198222754e-06, -5.911372050456217e-06, -5.213521866842439e-06, 8.357173387072054e-06, 8.197668789464496e-06, 2.243013888698414e-06, 7.971409789518113e-06, -5.90863826093963e-06, -5.209152217525797e-06, 2.8135370764489216e-06, 6.101660885407461e-06, 4.287271478860954e-06, 2.852934098562898e-06, -1.081819147632393e-06, -1.1982004593084628e-06, 2.978143072628614e-06, 4.624890628149974e-06, 3.2127033549828245e-06, 3.445123507964925e-06, -3.6922133335453714e-06, -4.83477145155976e-06, 2.8330403266740364e-06, 4.0536073423302685e-06, 4.14043601290631e-06, 3.23597932096201e-06, -3.580729476477538e-06, -4.643210598717123e-06, 1.419941063024807e-05, 3.0954589859465134e-05, 2.2342257720158122e-05, 2.095056135441059e-05, 2.6378564989473913e-06, 2.1668672871214205e-06, 5.115294853955347e-06, 1.0744902269093014e-05, 7.468923254570736e-06, 7.662133369355153e-06, -6.214624426908087e-06, -6.335050702918229e-06, 5.117110031989845e-06, 1.0741547303871847e-05, 7.467503739711319e-06, 7.662029362436934e-06, -6.220544716681307e-06, -6.349090870204924e-06, -5.852263604728355e-07, 5.283789537147448e-06, 1.5289686968290208e-06, 4.563242869153236e-07, 1.1171488373578838e-06, 1.085454954384776e-06, 1.4399629213445824e-06, 5.98936590068e-06, 4.056291311876347e-06, 3.798068945104328e-06, -3.1596785910514805e-06, -3.631462028321707e-06, 5.807421962735959e-06, 5.928023458871838e-06, 3.997945001497262e-06, 4.533849633621474e-06, -4.035209444863805e-06, -4.5198244086364774e-06, 2.4087958347036392e-06, 2.4416231954753488e-05, 8.681348247975418e-06, 1.1885963115749278e-05, 1.0397159878502404e-05, 1.0461451996237417e-05, -1.347976921963014e-06, 6.889279762670582e-06, 7.706060676383909e-06, -1.0087652766953076e-07, -2.400228418502749e-06, -1.6890701499081376e-06, -5.850696214564473e-06, -5.465092476562956e-06, 2.152407970314171e-06, -5.522281834964215e-06, -4.932786049683518e-06, -5.210006930184581e-06, 3.170254029815962e-06, 1.3063258285811049e-05, 9.616847318208497e-06, 5.109639939415713e-06, 9.159358664413476e-07, 1.023838466402573e-06, -2.911820558763372e-06, 1.7424704761833573e-05, 2.334478943240961e-05, 8.360018743228947e-06, 2.1231807769551313e-06, 2.1023181112537096e-06, 3.0269392511045264e-06, 1.6736539342423177e-05, 1.0644625490121866e-05, 5.0721404904789305e-06, 2.205809539750595e-06, 2.305787732226278e-06, 5.202555247049661e-06, 1.030886855984338e-05, 5.821426738162716e-06, 6.868814110317802e-06, -2.042725269617093e-06, -1.9329325321770168e-06, -1.0462772229474542e-06, 6.1741359625983006e-06, -1.2930826951042602e-06, -2.835431289970709e-06, -5.6451439969764345e-06, -5.527775409422465e-06, -4.377718417729033e-06, 5.0965229677484496e-06, 3.746310032387364e-06, -4.124557844453789e-06, 1.1622075629447762e-05, 1.1713043267980417e-05, -8.956641005344658e-07, 1.1629669268995328e-05, 5.948752975635096e-07, -4.2214185766739114e-07, 3.312115491333935e-06, 3.437233282397507e-06, 3.1210163532733653e-06, 1.265254041830222e-05, -6.319113698432272e-06, 1.0488252286046853e-05, 6.06344737946149e-06, 6.065387515540166e-06, 1.7523648918421108e-06, 1.134796566484629e-05, -4.18043925260274e-06, -2.3346087688321566e-06, 8.69854698858811e-06, 8.633913282890552e-06, -1.0281819017212628e-06, 1.5764359127386466e-05, 6.709731563045303e-06, 2.606253782660235e-06, -4.0088438728098825e-06, -4.249219557487032e-06]
total_var_F = [0.0006463018722716782, 2.2197245125323103e-07, 2.6905398119306834e-06, 1.1470549810047172e-06, 2.6316677775482718e-06, 6.921611841758418e-06, 2.90423210134814e-06, 1.0530146533298152e-06, 2.0581830652385054e-07, 2.1586963746702633e-07, 4.540881885457738e-07, 2.061651522864357e-07, 8.684092304990062e-07, 9.278027792126689e-07, 7.139156534363982e-06, 6.398093520513884e-07, 5.462521967637854e-07, 2.1146048696534204e-07, 1.2843296132822438e-07, 1.2576823328370732e-07, 8.306276167816785e-07, 8.378652765826798e-07, 7.137500495316649e-07, 5.358455133548894e-07, 3.608588262400653e-07, 6.17426822325067e-07, 8.384740816136469e-07, 8.565006783416916e-07, 7.136846964857714e-07, 5.357455444135566e-07, 3.608543374388285e-07, 6.173580857074772e-07, 8.384642092599057e-07, 8.564931348933601e-07, 1.9897866912161896e-07, 3.6186952797567695e-07, 4.1552766798986644e-07, 1.910780996017465e-07, 7.005458281385465e-07, 7.04995955694394e-07, 5.826777104794393e-07, 5.664758579918741e-07, 3.451861171370746e-07, 5.786850389884142e-07, 8.790131201921483e-07, 8.864067320423498e-07, 5.813960972217832e-07, 5.791869285005226e-07, 4.7124984073467846e-07, 5.816787237411236e-07, 8.850660302872128e-07, 8.934207189532497e-07, 1.1015432095200948e-06, 1.2154976846483134e-06, 1.048705042617473e-06, 1.0842183352098472e-06, 8.656597489116223e-07, 8.690137514234188e-07, 7.367184341087137e-07, 6.281574060740372e-07, 5.755707425335664e-07, 5.676351451465982e-07, 8.074790386959562e-07, 8.107856152260628e-07, 7.369830305209967e-07, 6.280461310843539e-07, 5.755528697220209e-07, 5.676641285818743e-07, 8.074750333131661e-07, 8.106932931348347e-07, 8.30089844544296e-08, 3.3430819689333844e-07, 1.874120018505639e-07, 1.6218476166456032e-07, 6.396521606481192e-07, 6.391048233500308e-07, 6.944945234140298e-07, 6.137341401697797e-07, 5.847081910119986e-07, 5.880321825513344e-07, 7.721004995603124e-07, 7.70307844194359e-07, 1.1876935439072346e-06, 6.330548529714046e-07, 5.998057011708582e-07, 6.317361587004808e-07, 7.683919073697902e-07, 7.665084475418896e-07, 1.0318712210032947e-06, 1.4540281759807384e-06, 9.366149271789897e-07, 1.0401472283121266e-06, 7.741273670656909e-07, 7.741335140826174e-07, 4.669207020001319e-07, 6.430522763991107e-07, 6.022164237669195e-07, 4.6936535066829755e-07, 8.326478563173284e-07, 8.4175736266052e-07, 9.835105763187093e-07, 9.763261779628845e-07, 9.207892230589763e-07, 9.764329289903354e-07, 8.449292454260774e-07, 8.350739513400801e-07, 7.208253420613218e-07, 9.215800084988262e-07, 7.657574286897422e-07, 7.120969340060728e-07, 7.436955435569723e-07, 7.40226940219072e-07, 1.1154052030851807e-06, 1.1278467960190628e-06, 1.2041204045586485e-06, 1.0788422214609578e-06, 7.706349826125246e-07, 7.735927627014384e-07, 5.869944979706454e-07, 1.0685493547558216e-06, 6.727875724178532e-07, 5.641549564338782e-07, 7.980576563907237e-07, 7.954129208954729e-07, 9.258413103447898e-07, 9.477388303368547e-07, 6.862706242912103e-07, 8.5916110729216e-07, 9.516702028410452e-07, 9.466227574499086e-07, 2.313097643205935e-07, 6.843712395945951e-07, 3.192381949929309e-07, 2.681590461485707e-07, 7.837695672804463e-07, 7.83482823735963e-07, 8.846319135778971e-07, 1.1325239598848567e-06, 1.0248078481741777e-06, 8.895197191180638e-07, 7.925461285004329e-07, 7.957520321866901e-07, 4.5980202906028273e-07, 9.777673816633332e-07, 6.133003073476937e-07, 5.702821710832103e-07, 7.717591641002086e-07, 7.728519974522257e-07, 1.1209495086846283e-06, 1.0634389098189911e-06, 8.930229121930895e-07, 1.0361848918641056e-06, 7.920530207480933e-07, 7.927451979182509e-07, 8.827021768483906e-07, 1.2514583178961497e-06, 6.756063702472179e-07, 8.013274026899159e-07, 8.17405836778166e-07, 8.187936273244898e-07, 1.022874321300039e-07, 1.5199612331628748e-06, 4.858334763364298e-07, 3.324756382068258e-07, 8.294480471318113e-07, 8.271061301814988e-07]
               '''

device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print('model GCN-XI:')
print('layer_num:', 1)
model = GCN(in_feats=166,
            out_feats=2,
            num_of_nodes=args.graph_size,
            mean_A=total_mean,
            var_A=total_var,
            var_F=total_var_F,
            layer_num=1).to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

criterion = torch.nn.CrossEntropyLoss()

exp = Experiment(model, optimizer, criterion,train_set,test_set,args)

for epoch in range(args.epochs):
    exp.train(epoch)
    exp.test()
