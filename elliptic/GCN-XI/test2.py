import torch
import argparse
import torch.nn as nn
from model import GraphConv,DGLError
class GCN(nn.Module):
    def __init__(self,
                 in_feats,
                 out_feats,
                 num_of_nodes,
                 mean_A,
                 var_A,
                 layer_num=1,
                 norm='none',
                 weight=True,
                 bias=True,
                 activation=None,
                 allow_zero_in_degree=False):
        super(GCN, self).__init__()
        if norm not in ('none', 'both', 'right', 'left'):
            raise DGLError(
                'Invalid norm value. Must be either "none", "both", "right" or "left".'
                ' But got "{}".'.format(norm))
        self._in_feats = in_feats
        self._out_feats = out_feats
        self._norm = norm
        self._allow_zero_in_degree = allow_zero_in_degree
        self.num_of_nodes = num_of_nodes
        self.mean = mean_A
        self.var = var_A
        self.layer_num = layer_num
        self.GraphConv = GraphConv(in_feats, in_feats, num_of_nodes, mean_A,
                                    var_A, norm, weight, bias, activation,
                                    allow_zero_in_degree)
        self.FC = nn.Sequential(nn.Linear(in_feats, out_feats),
                                nn.Flatten(0, -1),
                                nn.Linear(self.num_of_nodes * out_feats,
                                          out_feats))

    def forward(self, g, F, weight=None, edge_weight=None):
        x = self.GraphConv(g, F, weight, edge_weight)
        x = self.FC(x)
        return x
dir='../model_data/GCN-XI'
checkpoint = torch.load(dir)
total_mean, total_var = 3.683568769458099e-05, 4.561115092123831e-05
total_mean_F = [0.0021183301579994697, -3.7886682056970645e-06, 3.0536425783578385e-05, 1.5866319178304192e-05, 2.8589866381082663e-05, 3.4876941636988214e-05, 2.8914791489785524e-05, 5.587027235043108e-06, -5.426243393155047e-06, -4.4973350508664546e-06, 3.1728684864054945e-06, -5.303540240109642e-06, -3.4036893356103445e-06, -5.57182493452409e-07, 3.5277466104977566e-05, 2.3209233405869625e-06, 3.2514698905678534e-06, -4.6872862742370345e-06, -7.886672264355014e-06, -7.917602226862236e-06, -1.2809208580070815e-05, -1.2716018296409521e-05, 8.160100979761668e-06, 7.886444080849098e-06, 1.9300671619482103e-06, 7.658394288103046e-06, -5.826123275342848e-06, -5.1647211483971195e-06, 8.159185008066817e-06, 7.8839491070226e-06, 1.92965560909895e-06, 7.657389572370234e-06, -5.828201427046943e-06, -5.167295689034232e-06, 2.8318250821611123e-06, 6.1924700849491274e-06, 4.321181595213419e-06, 2.867032068317795e-06, -1.2846488698509923e-06, -1.3498266906013667e-06, 2.674816718986155e-06, 4.31164510090334e-06, 3.0409674198980115e-06, 3.1182033127945773e-06, -3.7201897948930777e-06, -4.874122153775018e-06, 2.5473491345293506e-06, 3.729553115621615e-06, 3.84595215151056e-06, 2.919614148906415e-06, -3.618466210383038e-06, -4.694111543022e-06, 1.3878881704164622e-05, 3.0636624738733066e-05, 2.208418686169648e-05, 2.0601325453152853e-05, 2.617028788568233e-06, 2.1725991289683526e-06, 5.061997321483616e-06, 1.0225702004068389e-05, 6.901207143408594e-06, 7.099020164964488e-06, -6.293243249259777e-06, -6.451988241255651e-06, 5.063824924339898e-06, 1.02223145589824e-05, 6.899819057238147e-06, 7.098943118286069e-06, -6.308312328094946e-06, -6.474110466741243e-06, -6.303209645569728e-07, 5.2638990187037516e-06, 1.4795303988661593e-06, 4.002476666805455e-07, 1.0011605625760613e-06, 9.544591006327044e-07, 1.1401427057107611e-06, 5.577398092748247e-06, 3.615395418312211e-06, 3.2777913857087453e-06, -3.3219789661067556e-06, -3.83903837451462e-06, 5.359498618461922e-06, 5.489910086878962e-06, 3.5459425257962404e-06, 3.984763645802095e-06, -4.236444138675062e-06, -4.760949963329562e-06, 1.9439131846780885e-06, 2.3949847013980303e-05, 8.05344534564808e-06, 1.1167809531008364e-05, 1.02281880043063e-05, 1.0277750493564796e-05, -1.4310817005193244e-06, 6.819230709623165e-06, 7.848851841874936e-06, -1.3549245776215638e-07, -2.44374986004389e-06, -1.802833785013021e-06, -6.729590490138597e-06, -6.349791347705012e-06, 2.1803683396079367e-06, -6.4006442202697225e-06, -4.977951651228545e-06, -5.2336246993905e-06, 2.7465729658996085e-06, 1.2721456955820583e-05, 9.642082280610985e-06, 4.735807176517087e-06, 1.0852440272442156e-06, 1.1877530796115832e-06, -3.14224794707136e-06, 1.716093648379723e-05, 2.3183626321092854e-05, 8.100362606925526e-06, 2.366444622755172e-06, 2.3099202478006164e-06, 2.936986441028857e-06, 1.6591499962365515e-05, 1.0493699681891548e-05, 4.975420113356189e-06, 2.3855874393689662e-06, 2.5228594114474573e-06, 5.3543736610704985e-06, 1.1052682305103425e-05, 6.491348786338596e-06, 7.30626627995199e-06, -2.3180124223367185e-06, -2.2435213309787934e-06, -1.188473722080102e-06, 5.926619935878643e-06, -1.5553863276567502e-06, -3.1095466784457106e-06, -5.727014115804099e-06, -5.638057299317368e-06, -5.1993617276726936e-06, 4.552319738385367e-06, 3.845370013889705e-06, -4.925631382057287e-06, 1.1922132452880119e-05, 1.2026229289484737e-05, -1.0768963279785546e-06, 1.2074225862132762e-05, 1.1767801729098557e-06, -1.1619945168885763e-07, 3.5641808780238203e-06, 3.6781522259251235e-06, 2.8756724393241006e-06, 1.30903395631317e-05, -5.906108233403186e-06, 1.050476465062772e-05, 6.3417845214880655e-06, 6.3364441564262995e-06, 1.948489789129324e-06, 1.2834325863024658e-05, -2.729728033001973e-06, -8.830768682977344e-07, 8.864060930466308e-06, 8.795446169383033e-06, -9.68630540756099e-07, 1.5649416025170543e-05, 6.7958627147612595e-06, 2.8053626978699363e-06, -3.979697434434923e-06, -4.2473301922475685e-06]
total_var_F = [0.0006493349452135278, 2.175639212425028e-07, 2.6868905843809325e-06, 1.1609480188942316e-06, 2.616570650475339e-06, 6.957712195483446e-06, 2.8927245033306994e-06, 1.0818770083769038e-06, 2.0185351312976657e-07, 2.1207605517588905e-07, 4.5708241027018686e-07, 2.0222932968520673e-07, 8.753845996166085e-07, 9.356704400607942e-07, 7.1787415579759355e-06, 6.412258317200422e-07, 5.58572777410231e-07, 2.054059909323248e-07, 1.2265368251866836e-07, 1.2072151015949271e-07, 8.419035741536057e-07, 8.491932517268825e-07, 7.109090418773087e-07, 5.309432293578949e-07, 3.4960473893157185e-07, 6.133049207916667e-07, 8.410338402803168e-07, 8.595077208315107e-07, 7.108403091072172e-07, 5.308400352899552e-07, 3.496001746542778e-07, 6.132334133942444e-07, 8.410220759711234e-07, 8.59493061248249e-07, 2.0061525518721937e-07, 3.677497185172173e-07, 4.201079234856301e-07, 1.9260761841418425e-07, 6.866721715047845e-07, 6.894556896303906e-07, 5.821502549677774e-07, 5.669405572830788e-07, 3.491395392282023e-07, 5.78299216921372e-07, 8.705570047681845e-07, 8.76830784915874e-07, 5.823090318764734e-07, 5.798893414890484e-07, 4.6315095553449985e-07, 5.823672557265246e-07, 8.763209718268421e-07, 8.830698104895307e-07, 1.1130102707850718e-06, 1.2282358280482884e-06, 1.0529635915838931e-06, 1.0949264534192201e-06, 8.644809397506703e-07, 8.671173341940851e-07, 7.406648987851892e-07, 6.191994999589456e-07, 5.636299349918899e-07, 5.565040532084706e-07, 8.190147697388718e-07, 8.220401785667298e-07, 7.409309258830412e-07, 6.190847905377816e-07, 5.636120099534077e-07, 5.5653236454551e-07, 8.190564376319595e-07, 8.21974580167053e-07, 8.08917911882055e-08, 3.294016213872004e-07, 1.809305862627351e-07, 1.5633657047955033e-07, 6.415069198859641e-07, 6.406499095508534e-07, 6.772667263225356e-07, 6.10961471376871e-07, 5.797583255571731e-07, 5.822270951006825e-07, 7.80611118286471e-07, 7.786938759078229e-07, 1.1231705134158713e-06, 6.333681856203133e-07, 5.990384419268891e-07, 6.268694565673774e-07, 7.767080196446046e-07, 7.750429783809389e-07, 1.0311987619673902e-06, 1.4669080459356488e-06, 9.340336455877144e-07, 1.0427532944601834e-06, 7.845232120130483e-07, 7.847635942213145e-07, 4.846689379377123e-07, 6.544361627297252e-07, 6.175085530346696e-07, 4.875453565515644e-07, 8.389886770724749e-07, 8.484983942788291e-07, 9.99778542666343e-07, 9.929693427229795e-07, 9.182695331145303e-07, 9.926874275670823e-07, 8.507337246116547e-07, 8.416822469424238e-07, 7.051602241230229e-07, 9.150452782156893e-07, 7.774395053636195e-07, 7.007881584034862e-07, 7.449326055461666e-07, 7.41287172875715e-07, 1.125876698744465e-06, 1.1415013598032813e-06, 1.2089389636750719e-06, 1.0910530908833341e-06, 7.758602503815411e-07, 7.784011729800128e-07, 5.875378582671116e-07, 1.05946016640694e-06, 6.603770544919727e-07, 5.658429639041697e-07, 8.049966207470144e-07, 8.027418505701506e-07, 1.0450835114776982e-06, 1.0898632928004954e-06, 8.12984678872957e-07, 9.778342005185823e-07, 9.618482511653214e-07, 9.561554343567099e-07, 2.1277476786019504e-07, 6.705699705558134e-07, 2.9708252945281685e-07, 2.469831319327717e-07, 7.958923070202029e-07, 7.957825514837613e-07, 9.010305238795134e-07, 1.150275724005803e-06, 1.0478082311895e-06, 9.055646176259219e-07, 8.033414051637516e-07, 8.068266835710362e-07, 4.6556562121189673e-07, 1.0092911985557481e-06, 6.570477643415937e-07, 5.970989512612426e-07, 7.833785321223925e-07, 7.843928844228527e-07, 1.1366741920773371e-06, 1.0860502948088588e-06, 9.100087304479684e-07, 1.0529304924940489e-06, 8.069567611806606e-07, 8.076967580652114e-07, 9.454654991373129e-07, 1.3111109969784005e-06, 7.360127632703061e-07, 8.755525645785471e-07, 8.308229768661617e-07, 8.319119078923245e-07, 1.360645675894565e-07, 1.4854893947041345e-06, 4.7028185494145907e-07, 3.4797368104211323e-07, 8.487154936406979e-07, 8.458412998815798e-07]
args = argparse.ArgumentParser()
args.add_argument('--learning_rate', type=float, default=3e-4)
args.add_argument('--epochs', type=int, default=150)
args.add_argument('--batch_size', type=int, default=24)
args.add_argument('--train_set_len', type=float, default=0.8)
args.add_argument('--shuffle', type=bool, default=True)
args.add_argument('--graph_size', type=int, default=950)
args = args.parse_args(args=[])
print(args)
print('model GCN-XI:')
model = GCN(in_feats=166,
            num_of_nodes=args.graph_size,
            out_feats=2,
            mean_A=total_mean,
            var_A=total_var,
            layer_num=2)
optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate)

criterion = torch.nn.CrossEntropyLoss()
model.load_state_dict(checkpoint['model'])

optimizer.load_state_dict(checkpoint['optimizer'])

start_epoch = checkpoint['epoch'] + 1